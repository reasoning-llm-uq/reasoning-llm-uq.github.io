<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>Reasoning about Uncertainty: Do Reasoning Models Know When They Don't Know?</title>

    <!--  =====  FONTS & ICONS  =====  -->
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css" />
    <link rel="icon" href="./static/icon.png" />

    <!--  =====  CSS  =====  -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/bulma/0.9.4/css/bulma.min.css" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/slick-carousel/1.8.1/slick.min.css" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/slick-carousel/1.8.1/slick-theme.min.css" />

    <style>
        /* --- GLOBAL --- */
        body {
            background: #fff;
            font-family: "Noto Sans", sans-serif;
            font-size: 16px;
            line-height: 1.5;
            color: #333;
        }

        section {
            padding: 2.5rem 0;
        }

        /* --- SECTION COLORS --- */
        .section-gray {
            background: #f7f7f7;
        }

        /* --- TYPOGRAPHY & LINKS --- */
        .task-title {
            margin-bottom: 1rem;
            font-weight: 600;
        }

        .publication-authors {
            margin-bottom: 1rem;
        }

        /* space below authors */
        .publication-authors a {
            color: #3273dc;
            text-decoration: none;
            white-space: nowrap;
        }

        .publication-authors a:hover {
            text-decoration: underline;
        }

        /* --- HERO & SPACING TWEAKS --- */
        .publication-title {
            margin-top: 0.0rem;
            padding-top: 0.0rem;
        }

        .hero {
            padding-top: 0.75rem;
            padding-bottom: 0.5rem;
        }

        /* tighter gap above overview */
        .hero-body {
            padding: 1.5rem;
        }

        #overview.section {
            padding-top: 0.75rem;
        }

        /* tighter gap below hero */
        .publication-links {
            margin-top: 1.5rem;
            margin-bottom: 1.5rem;
        }

        /* gap between authors/icons & icons/logo */
        figure.lab-logo {
            margin-top: 0.75rem;
        }

        /* gap between icons and Princeton logo */
        /* #sim-robot.section{padding-bottom:0.1rem;} gap above simulation section */
        #bibtex.section- {
            padding-top: 0rem;
        }

        /* gap above BibTeX section */

        /* --- SLICK ARROWS --- */
        .slick-prev:before,
        .slick-next:before {
            color: #3273dc;
            font-size: 32px;
        }

        /* --- THUMBNAILS --- */
        .video-thumb {
            cursor: pointer;
            border-radius: 8px;
            overflow: hidden;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
            margin: 0 8px;
        }

        .video-thumb video {
            width: 100%;
            height: auto;
            display: block;
        }

        /* --- STAND‑ALONE VIDEOS --- */
        .overview-video,
        .sim-video {
            width: 100%;
            height: auto;
            border-radius: 0;
            box-shadow: none;
            pointer-events: none;
        }

        /* --- LAYOUT HELPERS --- */
        .task-block {
            margin-bottom: 3rem;
        }

        .task-carousel,
        .task-carousel-exp {
            margin-top: 1rem;
        }

        /* --- CONTENT WIDTH CONSISTENCY --- */
        .content-container {
            max-width: 960px;
            margin: 0 auto;
        }

        /* --- PDF EMBED --- */
        .pdf-container {
            margin-top: 1.5rem;
        }

        .pdf-container object {
            width: 100%;
            height: 600px;
            border: none;
        }

        /* --- REAL EXP IMG --- */
        .real-exp-img {
            width: 100%;
            height: auto;
            max-width: 100%;
        }

        /* --- LINK COLOR (lighter blue, closer to default hyperlinks) --- */

        .publication-authors a:hover {
            text-decoration: underline;
        }

        /* --- HERO ↔ OVERVIEW GAP (shrink) --- */
        .hero {
            padding-bottom: 0rem;
        }

        /* was 0.5rem */
        #overview.section {
            padding-top: 0rem;
            padding-bottom: 0rem;
        }

        /* was 0.75rem */

        /* --- REAL-ROBOT ↔ SIMULATION GAP (shrink) --- */
        #real-robot.section {
            padding-top: 2.5rem;
        }

        /* default Bulma ≈2.5rem */
        #real-robot.section {
            padding-bottom: 2.5rem;
        }

        #sim-to-real-1 {
            margin-bottom: 0;
        }

        /* default Bulma ≈2.5rem */
        #sim-robot.section {
            padding-top: 2.5rem;
        }

        /* default Bulma ≈2.5rem */

        /* --- SIMULATION ↔ BIBTEX GAP (shrink) --- */
        #BibTeX.section {
            padding-top: 0.1rem;
        }

        /* tighten above BibTeX */
    </style>
</head>

<body>

    <!-- ===== HERO with title & authors ===== -->
    <section class="hero section">
        <div class="hero-body">
            <div class="container is-max-widescreen has-text-centered">
                <h1 class="title is-1 publication-title">
                    Reasoning about Uncertainty: <br />
                </h1>
                <h2 class="subtitle is-2 publication-title">
                    Do Reasoning Models Know When They Don't Know?
                </h2>

                <!-- ===== AUTHORS (three rows, no underscores) ===== -->

                <div class="is-size-5 publication-authors">
                    <div>
                        <span class="author-block"><a href="https://may0mei.github.io/">Zhiting&nbsp;Mei*</a></span>,
                        <span class="author-block"><a href="https://www.linkedin.com/in/-christina-zhang/">Christina&nbsp;Zhang</a></span>,
                        <span class="author-block"><a href="https://tenny-yinyijun.github.io/">Tenny&nbsp;Yin*</a></span>,
                        <span class="author-block"><a href="https://jlidard.github.io/">Justin&nbsp;Lidard</a></span>,
                        <span class="author-block"><a href="#">Ola&nbsp;Shorinwa*</a></span>,
                        <span class="author-block"><a href="https://irom-lab.princeton.edu/majumdar/">Anirudha&nbsp;Majumdar</a></span>
                    </div>
                </div>

                <div>
                    <sup>&#42;</sup>Equal Contribution.
                </div>

                <!-- ===== RESOURCE ICONS ===== -->
                <div class="publication-links">
                    <!-- <a href="static/paper.pdf" class="external-link button is-normal is-rounded is-dark" target="_blank"
                        rel="noopener">
                        <span class="icon"><i class="fas fa-file-pdf"></i></span><span>Paper</span>
                    </a> -->
                    <a href="https://arxiv.org/abs/2506.18183" class="external-link button is-normal is-rounded is-dark"
                        target="_blank" rel="noopener">
                        <span class="icon"><i class="ai ai-arxiv"></i></span><span>arXiv</span>
                    </a>
                    <!-- <a href="https://www.youtube.com/watch?v=i1qSlALio-o"
                        class="external-link button is-normal is-rounded is-dark" target="_blank" rel="noopener">
                        <span class="icon"><i class="fab fa-youtube"></i></span><span>Video</span>
                    </a> -->
                    <a href="#" class="external-link button is-normal is-rounded is-dark" target="_blank"
                        rel="noopener">
                        <span class="icon"><i class="fab fa-github"></i></span><span>Code (Coming Soon!)</span>
                    </a>
                </div>

                <!-- ===== LAB LOGO ===== -->

                <figure class="image is-inline-block lab-logo">
                    <img src="static/irom_princeton.png" alt="IROM Lab logo" style="max-width:500px;">
                </figure>

            </div>
        </div>
    </section>

    <!-- ===== TL; DR (inline style) ===== -->
    <div class="container is-max-desktop content-container has-text-centered" style="margin-bottom:1.5rem;">
        <div style="background:#f7f7f7; border-radius:16px; padding:1.25em 1.5em; display:inline-block; box-shadow:0 1px 4px rgba(0,0,0,0.04);">
            <strong><span style="color:#910101; font-size:1.5em;">TL;DR:</span></strong>
            <span style="font-size:1.5em;">
                Reasoning models are generally <em>overconfident</em> and become even more overconfident with deeper reasoning.
                To improve their calibration, we introduce <em><span style="color:#910101;">introspective uncertainty quantification</span></em> (UQ), which allows reasoning models to explicitly reason about their chain-of-thought traces.
            </span>
        </div>
    </div>

    <!-- ===== OVERVIEW ===== -->
    <section id="overview" class="section">
        <div class="container is-max-desktop content-container has-text-centered">
            <video class="overview-video" autoplay loop muted playsinline poster="static/thumbnails/banner.jpg">
                <source src="static/videos/anchor.mp4" type="video/mp4" />
                Your browser does not support the video tag.
            </video>
        </div>
    </section>

    <!-- ===== ABSTRACT ===== -->
    <section id="abstract" class="section">
        <div class="container is-max-desktop content-container">
            <h2 class="title is-3 has-text-centered">Abstract</h2>
            <p> 
                Reasoning language models have set state-of-the-art (SOTA) records on many challenging benchmarks, 
                enabled by multi-step reasoning induced using reinforcement learning. 
                However, like previous language models, reasoning models are prone to generating confident, 
                plausible responses that are incorrect (hallucinations). 
                Knowing when and how much to trust these models is critical to the safe deployment of reasoning models 
                in real-world applications. To this end, we explore <em>uncertainty quantification</em> of reasoning models in this work. 
                Specifically, we ask three fundamental questions: 
                First, are reasoning models <em>well-calibrated?</em> 
                Second, does <em>deeper reasoning</em> improve model calibration? 
                Finally, inspired by humans' innate ability to double-check their thought processes to 
                verify the validity of their answers and their confidence, we ask: 
                can reasoning models improve their calibration by <em>explicitly reasoning</em> about their chain-of-thought traces? 
                We introduce <b><em><span style="color:#910101;">introspective uncertainty quantification</span></em></b> (UQ) to explore this direction. 
                In extensive evaluations on SOTA reasoning models across a broad range of benchmarks, 
                we find that reasoning models: (i) <em> are typically overconfident, with self-verbalized confidence estimates often 
                greater than 85% particularly for incorrect responses</em>, (ii) <em> become even more overconfident with deeper reasoning</em>, 
                and (iii) <em> can become better calibrated through introspection </em> (e.g., o3-Mini and DeepSeek R1) <em> but not uniformly </em>
                (e.g., Claude 3.7 Sonnet becomes more poorly calibrated). Lastly, we conclude with important research directions 
                to design necessary UQ benchmarks and improve the calibration of reasoning models.
            </p>
            <!-- <br>
      <video id="summary-video" class="shadow" controls preload="metadata" width="100%" poster="static/thumbnails/talk_video.jpg">
        <source src="static/videos/talk_video.mp4" type="video/mp4" />
        Your browser does not support the video tag.
      </video> -->
        </div>
    </section>


    <!-- ===== Body text ===== -->
    <section id="body" class="section">
        <div class="container is-max-desktop content-container">
            <h2 class="title is-3 has-text-centered">Are reasoning models calibrated?</h2>
            <img src="static/images/calibration_of_reasoning_models_avg_prompts.png" class="real-exp-img" alt="PyBullet Evaluation." />
            <br>

            <p>
                Recent benchmarks reveal that reasoning models are well-calibrated on older datasets like ARC-Challenge and MMLU, but their calibration drops on newer, more challenging benchmarks such as StrategyQA, GPQA, and SimpleQA. This suggests that as benchmarks evolve, models struggle to maintain reliable confidence estimates. Calibration on one dataset does not guarantee good calibration on others, especially as models achieve near-perfect accuracy and overconfidence becomes harder to detect. As a result, continually updating benchmarks is crucial for accurately assessing and improving uncertainty quantification in reasoning models.
            </p>
            </p>

            <!-- <img src="static/images/reasoning/Reliability_hard-1.png" class="real-exp-img" alt="Introspective UQ." /> -->

            <br>
            <p> 
            <strong>Is accuracy correlated with calibration in reasoning models?</strong> The ECE is strongly correlated with accuracy
            in state-of-the-art reasoning models. In essence, the self-verbalized confidence of reasoning models is generally not
            trustworthy in problems where they achieve low accuracy. Remarkably, Claude remains well-calibrated even in these
            low-accuracy settings. We observe the same trends in the correlation between accuracy and the MCE.
            </p>
            <img src="static/images/reasoning/linear_fit_correlation_accuracy_calibration-1.png" class="real-exp-img" alt="Introspective UQ." />

            <!-- <img src="static/images/reasoning/calibration_vs_prompts-1.png" class="real-exp-img" alt="Introspective UQ." /> -->

            <br>
            <p>
                <strong>Does increasing reasoning depth improve calibration?</strong> Deeper reasoning leads to overall higher accuracy;
                however, as the accuracy of these models saturate, reasoning models become even more overconfident.
            </p>
            </p>
            <img src="static/images/reasoning/calibration_vs_reasoning_effort-1.png" class="real-exp-img" alt="Introspective UQ." />

            <br>
            <p>
                <strong>Deeper Reasoning vs. Underconfidence/Overconfidence?</strong> Reasoning models become more overconfident
                with deeper reasoning, evidenced by the increase in the density of samples in higher-confidence bins
                (e.g., the 90%-95% interval) without a corresponding increase in accuracy.
            </p>
            <img src="static/images/reasoning/reliability_reasoning_effort-1.png" class="real-exp-img" alt="Introspective UQ." />

        </div>
    </section>

    <!-- ===== Introspective UQ ===== -->
    <section id="introspective" class="section">
        <div class="container is-max-desktop content-container">
            <h2 class="title is-3 has-text-centered">Introspective UQ: Reasoning about Confidence</h2>
            <video class="overview-video" autoplay loop muted playsinline poster="static/thumbnails/gaussian-splat.jpg">
                <source src="static/videos/introspective.mp4" type="video/mp4" />
                Your browser does not support the video tag.
            </video>
            <br>
            <p>
                In this section, we explore whether reasoning models can become <em>better calibrated</em> by explicitly reasoning about their own uncertainty. 
                We introduce a <em>two-stage <span style="color:#910101;">introspective uncertainty quantification (UQ)</span></em> process: First, the model answers a question and provides its confidence.
                Then, a fresh instance of the model reviews the reasoning trace from the first stage, searching for flaws and updating its confidence estimate.
                We test three prompt strategies—<em><b>IUQ-Low</b></em>, <em><b>IUQ-Medium</b></em>, and <em><b>IUQ-High</b></em>—each increasing in conservativeness and critical self-reflection.
                Notably, the model is not asked to change its answer, only to reassess its confidence based on introspection. 
                This approach helps us understand if models can <em>self-correct overconfidence</em> by critically evaluating their own reasoning, a step toward safer and more reliable AI systems. (<em>Gemini</em> is excluded from these results due to limited support for reasoning trace analysis.)
            </p>

            <br>
            <p>  First, we show the effect of introspective UQ on the calibration of the reasoning models on average. </p>
            <img src="static/images/introspective/introspective_reasoning_calibration_average.png" class="real-exp-img" alt="Introspective UQ." />
            <br>
            <p>
                Calibration of reasoning models improves with introspective uncertainty quantification, particularly in challenging datasets, e.g., SimpleQA, with more conservative prompts, e.g., IUQ-Medium and IUQ-High.           
            </p>
            <br>

            <br>
            <p> Next, we show the effect of introspective UQ on the overconfidence/underconfidence of reasoning models on average. </p>
            <img src="static/images/introspective/reliability_introspective_reasoning_average.png" class="real-exp-img" alt="Introspective UQ." />
            <br>
            <p>
                More critical introspection, e.g., IUQ-Medium and IUQ-High, improves the calibration of reasoning models, mitigating model overconfidence, unlike IUQ-Low.
            </p>
            <br>

            <p> The benefits of introspective UQ can also be seen in Deepseek and o3-mini individually. However, since Claude is already well calibrated, introspective UQ can introduce unnecessary conservatism. </p>
            <img src="static/images/introspective/introspective_reasoning_calibration_models.png" class="real-exp-img" alt="Introspective UQ." />
            <br>
            <p>
                Introspective UQ improves the calibration of DeepSeek and o3-Mini, especially in the challenging dataset SimpleQA, but degrades the calibration of Claude.
            </p>        

            <br> 
            <p> Through introspection, DeepSeek and o3-Mini become less overconfident, especially with IUQ-Medium and IUQ-High, unlike Claude, which becomes more overconfident. </p>
            <img src="static/images/introspective/reliability_introspective_reasoning_models.png" class="real-exp-img" alt="Introspective UQ." />
            <br>
        </div>
    </section>


    <section class="section" id="BibTeX">
        <div class="container is-max-desktop content">
            <h2 class="title">BibTeX</h2>
            <pre><code>
                @misc{mei2025reasoninguncertaintyreasoningmodels,
                title={Reasoning about Uncertainty: Do Reasoning Models Know When They Don't Know?}, 
                author={Zhiting Mei and Christina Zhang and Tenny Yin and Justin Lidard and Ola Shorinwa and Anirudha Majumdar},
                year={2025},
                eprint={2506.18183},
                archivePrefix={arXiv},
                primaryClass={cs.AI},
                url={https://arxiv.org/abs/2506.18183}, 
                }
        </code></pre>
        </div>
    </section>


    <br>
    <center class="is-size-10">
        The website design was adapted from <a href="https://nerfies.github.io" class="external-link"><span
                class="dnerf">Nerfies</span></a>.
    </center>
    <br>

    <!-- ===== SCRIPTS ===== -->
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/slick-carousel/1.8.1/slick.min.js"></script>
    <script>
        $(function () {
            // Init carousels (only those still using .task-carousel wrappers)
            $('.task-carousel').slick({
                slidesToShow: 4,
                slidesToScroll: 1,
                infinite: false,
                arrows: true,
                dots: false,
                lazyLoad: 'ondemand',
                touchMove: true,
                responsive: [
                    { breakpoint: 1024, settings: { slidesToShow: 3 } },
                    { breakpoint: 768, settings: { slidesToShow: 2 } },
                    { breakpoint: 480, settings: { slidesToShow: 1 } }
                ]
            });

            // Horizontal track‑pad scroll → carousel navigation; let vertical scroll bubble up
            $('.task-carousel').on('wheel', function (e) {
                const deltaX = e.originalEvent.deltaX;
                const deltaY = e.originalEvent.deltaY;
                if (Math.abs(deltaX) > Math.abs(deltaY)) {
                    e.preventDefault();
                    $(this).slick(deltaX < 0 ? 'slickPrev' : 'slickNext');
                }
            });

            $('.task-carousel-exp').slick({
                slidesToShow: 3,
                slidesToScroll: 1,
                infinite: false,
                arrows: true,
                dots: false,
                lazyLoad: 'ondemand',
                touchMove: true,
                responsive: [
                    { breakpoint: 1024, settings: { slidesToShow: 1 } },
                    { breakpoint: 768, settings: { slidesToShow: 1 } },
                    { breakpoint: 480, settings: { slidesToShow: 1 } }
                ]
            });

            // Horizontal track‑pad scroll → carousel navigation; let vertical scroll bubble up
            $('.task-carousel').on('wheel', function (e) {
                const deltaX = e.originalEvent.deltaX;
                const deltaY = e.originalEvent.deltaY;
                if (Math.abs(deltaX) > Math.abs(deltaY)) {
                    e.preventDefault();
                    $(this).slick(deltaX < 0 ? 'slickPrev' : 'slickNext');
                }
            });
            $('.task-carousel-exp').on('wheel', function (e) {
                const deltaX = e.originalEvent.deltaX;
                const deltaY = e.originalEvent.deltaY;
                if (Math.abs(deltaX) > Math.abs(deltaY)) {
                    e.preventDefault();
                    $(this).slick(deltaX < 0 ? 'slickPrev' : 'slickNext');
                }
            });

        });
    </script>
</body>

</html>